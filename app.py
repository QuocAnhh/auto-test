# -*- coding: utf-8 -*-
import json
from datetime import datetime

import streamlit as st
import os
from dotenv import load_dotenv

from busqa.api_client import fetch_messages
from busqa.normalize import normalize_messages, build_transcript
from busqa.metrics import compute_latency_metrics
from busqa.prompting import SYSTEM_PROMPT, build_user_prompt
from busqa.llm_client import call_llm
from busqa.evaluator import coerce_llm_json
from busqa.rubrics import ALLOWED_INTENTS
from busqa.utils import safe_parse_headers

DEFAULT_BASE_URL = "http://103.141.140.243:14496"

st.set_page_config(page_title="Bus QA LLM Evaluator (Modular)", page_icon="üöå", layout="centered")
st.title("üöå Bus QA LLM Evaluator ‚Äî Modular Version")

load_dotenv()

with st.sidebar:
    st.subheader("API h·ªôi tho·∫°i")
    base_url = st.text_input("BASE_URL", value=DEFAULT_BASE_URL)
    conv_id = st.text_input("conversation_id", value="", placeholder="Nh·∫≠p conversation_id")
    headers_raw = st.text_area("Headers (JSON, optional)", value="", height=80, placeholder='{"Authorization":"Bearer xxx"}')
    st.markdown("---")
    st.subheader("C·∫•u h√¨nh LLM (c·ªë ƒë·ªãnh)")
    st.caption("Model: gemini-2.5-flash | API key l·∫•y t·ª´ file .env")
    llm_base_url = st.text_input("LLM Base URL (optional)", value="", help="ƒê·ªÉ tr·ªëng n·∫øu d√πng OpenAI ch√≠nh th·ªëng")
    temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.1)
    st.markdown("---")
    max_chars = st.number_input("Gi·ªõi h·∫°n k√Ω t·ª± transcript", min_value=2000, max_value=200000, value=24000, step=1000)

colA, colB = st.columns(2)
with colA:
    go = st.button("üöÄ Ch·∫•m ƒëi·ªÉm")
with colB:
    show_raw = st.toggle("Hi·ªán transcript & JSON th√¥", value=False)

st.caption(f"Intent h·ª£p l·ªá: {', '.join(ALLOWED_INTENTS)}")

if go:
    st.info("[LOG] B·∫Øt ƒë·∫ßu ch·∫•m ƒëi·ªÉm...")
    if not conv_id.strip():
        st.error("H√£y nh·∫≠p conversation_id.")
        st.stop()

    llm_api_key = os.getenv("GEMINI_API_KEY", "")
    llm_model = "gemini-2.5-flash"
    if not llm_api_key.strip():
        st.error("Kh√¥ng t√¨m th·∫•y GEMINI_API_KEY trong file .env ho·∫∑c bi·∫øn m√¥i tr∆∞·ªùng.")
        st.stop()

    # 1) L·∫•y d·ªØ li·ªáu
    try:
        st.info(f"[LOG] G·ªçi API l·∫•y h·ªôi tho·∫°i: {base_url}/api/conversations/{conv_id.strip()}/messages")
        raw = fetch_messages(base_url, conv_id.strip(), safe_parse_headers(headers_raw))
        st.write("[LOG] D·ªØ li·ªáu API tr·∫£ v·ªÅ:", raw)
    except Exception as e:
        st.error(f"L·ªói API: {e}")
        st.stop()

    if show_raw:
        st.subheader("Raw JSON")
        st.code(json.dumps(raw, ensure_ascii=False, indent=2))

    # 2) Chu·∫©n ho√° & metrics
    st.info("[LOG] Chu·∫©n ho√° d·ªØ li·ªáu h·ªôi tho·∫°i...")
    messages = normalize_messages(raw)
    st.write("[LOG] Messages chu·∫©n ho√°:", messages)
    if not messages:
        st.warning("Kh√¥ng c√≥ tin nh·∫Øn n√†o.")
        st.stop()

    st.info("[LOG] X√¢y transcript v√† t√≠nh metrics...")
    transcript = build_transcript(messages, max_chars=max_chars)
    metrics = compute_latency_metrics(messages)
    st.write("[LOG] Metrics:", metrics)
    st.write("[LOG] Transcript (preview):", transcript[:500])

    if show_raw:
        st.subheader("Transcript")
        st.text(transcript)
        st.subheader("Metrics")
        st.code(json.dumps(metrics, ensure_ascii=False, indent=2))

    # 3) Prompting
    st.info("[LOG] Build user prompt...")
    user_prompt = build_user_prompt(metrics, transcript)
    st.write("[LOG] User prompt:", user_prompt)

    # 4) G·ªçi LLM
    try:
        st.info("[LOG] G·ªçi LLM...")
        llm_json = call_llm(
            api_key=llm_api_key,
            model=llm_model.strip(),
            system_prompt=SYSTEM_PROMPT,
            user_prompt=user_prompt,
            base_url=llm_base_url.strip() or None,
            temperature=temperature,
        )
        st.write("[LOG] LLM tr·∫£ v·ªÅ:", llm_json)
    except Exception as e:
        st.error(f"L·ªói LLM: {e}")
        st.stop()

    # 5) Chu·∫©n ho√° k·∫øt qu·∫£
    try:
        st.info("[LOG] Chu·∫©n ho√° k·∫øt qu·∫£ LLM...")
        result = coerce_llm_json(llm_json)
        st.write("[LOG] K·∫øt qu·∫£ chu·∫©n ho√°:", result)
    except Exception as e:
        st.error(f"K·∫øt qu·∫£ kh√¥ng h·ª£p l·ªá: {e}")
        st.stop()

    # 6) Hi·ªÉn th·ªã
    st.success("ƒê√£ ch·∫•m xong b·∫±ng LLM ‚úÖ")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Intent", result.detected_intent)
    with col2:
        st.metric("Confidence", f"{round(result.confidence*100,1)}%")
    with col3:
        st.metric("ƒêi·ªÉm t·ªïng", f"{round(result.total_score,1)}/100")
    st.markdown(f"**Nh√£n:** {result.label}")
    st.markdown(f"**Final:** {result.final_comment or '(kh√¥ng c√≥)'}")

    st.subheader("üìã Chi ti·∫øt ti√™u ch√≠")
    for name, obj in result.criteria.items():
        st.markdown(f"**{name}** ‚Äî {round(float(obj.get('score',0.0)),1)}/100")
        st.progress(min(max(float(obj.get('score',0.0))/100.0, 0.0), 1.0))
        note = obj.get("note")
        if note:
            st.caption(f"Note: {note}")

    cols = st.columns(3)
    with cols[0]:
        st.markdown("**üè∑Ô∏è Tags**")
        st.write(", ".join(result.tags) if result.tags else "‚Äî")
    with cols[1]:
        st.markdown("**‚ö†Ô∏è Risks**")
        if result.risks:
            for r in result.risks:
                st.write(f"- {r}")
        else:
            st.write("‚Äî")
    with cols[2]:
        st.markdown("**üõ†Ô∏è Suggestions**")
        if result.suggestions:
            for s in result.suggestions:
                st.write(f"- {s}")
        else:
            st.write("‚Äî")

    export = {
        "conversation_id": conv_id.strip(),
        "base_url": base_url,
        "evaluated_at": datetime.utcnow().isoformat() + "Z",
        "llm_model": llm_model.strip(),
        "result": result.model_dump(),
        "metrics": metrics,
        "transcript_preview": transcript[:2000],
    }
    st.download_button(
        "‚¨áÔ∏è T·∫£i JSON k·∫øt qu·∫£",
        data=json.dumps(export, ensure_ascii=False, indent=2),
        file_name=f"evaluation_{conv_id.strip()}.json",
        mime="application/json"
    )