import json
from datetime import datetime

import streamlit as st
import os
from dotenv import load_dotenv

from busqa.api_client import fetch_messages
from busqa.normalize import normalize_messages, build_transcript
from busqa.metrics import compute_latency_metrics, compute_additional_metrics, compute_policy_violations_count, filter_non_null_metrics
from busqa.prompt_loader import load_unified_rubrics
from busqa.brand_specs import load_brand_prompt
from busqa.prompting import build_system_prompt_unified, build_user_instruction
from busqa.llm_client import call_llm
from busqa.evaluator import coerce_llm_json_unified
from busqa.utils import safe_parse_headers

DEFAULT_BASE_URL = "http://103.141.140.243:14496"

st.set_page_config(page_title="Bus QA LLM Evaluator (Modular)", page_icon="üöå", layout="centered")
st.title("üöå Bus QA LLM Evaluator ‚Äî Modular Version")

load_dotenv()

with st.sidebar:
    st.subheader("API h·ªôi tho·∫°i")
    base_url = st.text_input("BASE_URL", value=DEFAULT_BASE_URL)
    conv_id = st.text_input("conversation_id", value="", placeholder="Nh·∫≠p conversation_id")
    headers_raw = st.text_area("Headers (JSON, optional)", value="", height=80, placeholder='{"Authorization":"Bearer xxx"}')
    st.markdown("---")
    st.subheader("Brand Configuration")
    brand_options = ["son_hai", "long_van"]
    selected_brand = st.selectbox("Ch·ªçn brand", brand_options)
    
    # Load and display brand policy
    try:
        brand_path = f"brands/{selected_brand}/prompt.md"
        brand_prompt_text, brand_policy = load_brand_prompt(brand_path)
        
        st.caption("**Brand Policy Flags:**")
        st.write(f"‚Ä¢ C·∫•m thu SƒêT: {brand_policy.forbid_phone_collect}")
        st.write(f"‚Ä¢ Ch√†o c·ªë ƒë·ªãnh: {brand_policy.require_fixed_greeting}")
        st.write(f"‚Ä¢ C·∫•m t√≥m t·∫Øt: {brand_policy.ban_full_summary}")
        st.write(f"‚Ä¢ Max openers: {brand_policy.max_prompted_openers}")
        st.write(f"‚Ä¢ ƒê·ªçc ti·ªÅn b·∫±ng ch·ªØ: {brand_policy.read_money_in_words}")
    except Exception as e:
        st.error(f"L·ªói load brand: {e}")
        brand_prompt_text = ""
        brand_policy = None
    
    st.markdown("---")
    st.subheader("Diagnostics Configuration")
    apply_diagnostics_checkbox = st.checkbox("Apply diagnostic penalties", value=True, 
                                           help="B·∫≠t/t·∫Øt √°p d·ª•ng ph·∫°t ƒëi·ªÉm t·ª´ diagnostics")
    st.session_state['apply_diagnostics'] = apply_diagnostics_checkbox
    
    # Show diagnostics info
    try:
        from busqa.prompt_loader import load_diagnostics_config
        diag_cfg = load_diagnostics_config()
        st.caption(f"**Diagnostics loaded:** {len(diag_cfg.get('operational_readiness', []))} OR + {len(diag_cfg.get('risk_compliance', []))} RC rules")
    except Exception as e:
        st.error(f"L·ªói load diagnostics: {e}")
    
    st.markdown("---")
    st.subheader("C·∫•u h√¨nh LLM")
    st.caption("Model: gemini-2.5-flash | API key l·∫•y t·ª´ file .env")
    llm_base_url = st.text_input("LLM Base URL (optional)", value="", help="ƒê·ªÉ tr·ªëng n·∫øu d√πng OpenAI ch√≠nh th·ªëng")
    temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.1)
    st.markdown("---")
    max_chars = st.number_input("Gi·ªõi h·∫°n k√Ω t·ª± transcript", min_value=2000, max_value=200000, value=24000, step=1000)

colA, colB = st.columns(2)
with colA:
    go = st.button("üöÄ Ch·∫•m ƒëi·ªÉm")
with colB:
    show_raw = st.toggle("Hi·ªán transcript & JSON th√¥", value=False)

# Load unified rubrics to show available flows
try:
    rubrics_cfg = load_unified_rubrics()
    flows = list(rubrics_cfg.get('flows_slots', {}).keys())
    st.caption(f"Flows h·ª£p l·ªá: {', '.join(flows)}")
except Exception as e:
    st.error(f"L·ªói load rubrics: {e}")
    rubrics_cfg = None

# Load diagnostics config (outside sidebar to be accessible)
apply_diagnostics = st.session_state.get('apply_diagnostics', True)
try:
    from busqa.prompt_loader import load_diagnostics_config
    diagnostics_cfg = load_diagnostics_config()
except Exception as e:
    st.error(f"Warning: Could not load diagnostics config: {e}")
    diagnostics_cfg = None
    apply_diagnostics = False

if go:
    st.info("[LOG] B·∫Øt ƒë·∫ßu ch·∫•m ƒëi·ªÉm...")
    if not conv_id.strip():
        st.error("H√£y nh·∫≠p conversation_id.")
        st.stop()

    if not rubrics_cfg:
        st.error("Kh√¥ng th·ªÉ load unified rubrics.")
        st.stop()
        
    if not brand_policy:
        st.error("Kh√¥ng th·ªÉ load brand policy.")
        st.stop()

    llm_api_key = os.getenv("GEMINI_API_KEY", "")
    llm_model = "gemini-2.5-flash"
    if not llm_api_key.strip():
        st.error("Kh√¥ng t√¨m th·∫•y GEMINI_API_KEY trong file .env ho·∫∑c bi·∫øn m√¥i tr∆∞·ªùng.")
        st.stop()

    # 1) L·∫•y d·ªØ li·ªáu
    try:
        st.info(f"[LOG] G·ªçi API l·∫•y h·ªôi tho·∫°i: {base_url}/api/conversations/{conv_id.strip()}/messages")
        raw = fetch_messages(base_url, conv_id.strip(), safe_parse_headers(headers_raw))
        st.write("[LOG] D·ªØ li·ªáu API tr·∫£ v·ªÅ:", raw)
    except Exception as e:
        st.error(f"L·ªói API: {e}")
        st.stop()

    if show_raw:
        st.subheader("Raw JSON")
        st.code(json.dumps(raw, ensure_ascii=False, indent=2))

    # 2) Chu·∫©n ho√° & metrics
    st.info("[LOG] Chu·∫©n ho√° d·ªØ li·ªáu h·ªôi tho·∫°i...")
    messages = normalize_messages(raw)
    st.write("[LOG] Messages chu·∫©n ho√°:", messages)
    if not messages:
        st.warning("Kh√¥ng c√≥ tin nh·∫Øn n√†o.")
        st.stop()

    st.info("[LOG] X√¢y transcript v√† t√≠nh metrics...")
    transcript = build_transcript(messages, max_chars=max_chars)
    metrics = compute_latency_metrics(messages)
    additional_metrics = compute_additional_metrics(messages)
    
    # Compute policy violations
    policy_violations_count = compute_policy_violations_count(messages, brand_policy)
    additional_metrics["policy_violations"] = policy_violations_count
    
    metrics.update(additional_metrics)
    
    # Filter out None values for LLM prompt
    metrics_for_llm = filter_non_null_metrics(metrics)
    
    st.write("[LOG] Metrics:", metrics)
    st.write("[LOG] Transcript (preview):", transcript[:500])

    if show_raw:
        st.subheader("Transcript")
        st.text(transcript)
        st.subheader("Metrics")
        st.code(json.dumps(metrics, ensure_ascii=False, indent=2))

    # 3) Prompting - Build unified prompts
    st.info("[LOG] Build unified prompts...")
    system_prompt = build_system_prompt_unified(rubrics_cfg, brand_policy, brand_prompt_text)
    user_prompt = build_user_instruction(metrics_for_llm, transcript, rubrics_cfg)
    
    st.write("[LOG] System prompt (preview):", system_prompt[:500] + "...")
    st.write("[LOG] User prompt (preview):", user_prompt[:500] + "...")

    # 4) G·ªçi LLM
    try:
        st.info("[LOG] G·ªçi LLM...")
        llm_json = call_llm(
            api_key=llm_api_key,
            model=llm_model.strip(),
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            base_url=llm_base_url.strip() or None,
            temperature=temperature,
        )
        st.write("[LOG] LLM tr·∫£ v·ªÅ:", llm_json)
    except Exception as e:
        st.error(f"L·ªói LLM: {e}")
        st.stop()

    # 5) Chu·∫©n ho√° k·∫øt qu·∫£ v·ªõi unified system
    try:
        st.info("[LOG] Chu·∫©n ho√° k·∫øt qu·∫£ LLM...")
        diagnostics_hits = metrics.get("diagnostics", {}) if apply_diagnostics else {}
        
        result = coerce_llm_json_unified(
            llm_json, 
            rubrics_cfg=rubrics_cfg,
            brand_policy=brand_policy, 
            messages=messages, 
            transcript=transcript, 
            metrics=metrics,
            diagnostics_cfg=diagnostics_cfg if apply_diagnostics else None,
            diagnostics_hits=diagnostics_hits
        )
        st.write("[LOG] K·∫øt qu·∫£ chu·∫©n ho√°:", result)
    except Exception as e:
        st.error(f"K·∫øt qu·∫£ kh√¥ng h·ª£p l·ªá: {e}")
        st.stop()

    # 6) Hi·ªÉn th·ªã k·∫øt qu·∫£ unified
    st.success("ƒê√£ ch·∫•m xong b·∫±ng Unified Rubric System ‚úÖ")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Flow", result.detected_flow)
    with col2:
        st.metric("Confidence", f"{round(result.confidence*100,1)}%")
    with col3:
        st.metric("ƒêi·ªÉm t·ªïng", f"{round(result.total_score,1)}/100")
    st.markdown(f"**Nh√£n:** {result.label}")
    st.markdown(f"**Brand:** {selected_brand}")
    st.markdown(f"**Final:** {result.final_comment or '(kh√¥ng c√≥)'}")

    st.subheader("üìã Chi ti·∫øt ti√™u ch√≠")
    for name, obj in result.criteria.items():
        st.markdown(f"**{name}** ‚Äî {round(float(obj.get('score',0.0)),1)}/100")
        st.progress(min(max(float(obj.get('score',0.0))/100.0, 0.0), 1.0))
        note = obj.get("note")
        if note:
            st.caption(f"Note: {note}")

    cols = st.columns(3)
    with cols[0]:
        st.markdown("**üè∑Ô∏è Tags**")
        st.write(", ".join(result.tags) if result.tags else "‚Äî")
    with cols[1]:
        st.markdown("**‚ö†Ô∏è Risks**")
        if result.risks:
            for r in result.risks:
                st.write(f"- {r}")
        else:
            st.write("‚Äî")
    with cols[2]:
        st.markdown("**üõ†Ô∏è Suggestions**")
        if result.suggestions:
            for s in result.suggestions:
                st.write(f"- {s}")
        else:
            st.write("‚Äî")

    # Display diagnostics hits
    if diagnostics_cfg and apply_diagnostics:
        diagnostics_hits = metrics.get("diagnostics", {})
        if diagnostics_hits:
            st.subheader("üîç Diagnostics Hits")
            
            col_or, col_rc = st.columns(2)
            
            with col_or:
                st.markdown("**Operational Readiness**")
                or_hits = diagnostics_hits.get("operational_readiness", [])
                if or_hits:
                    for hit in or_hits:
                        st.markdown(f"‚Ä¢ **{hit['key']}**")
                        if hit.get('evidence'):
                            st.caption(f"Evidence: {hit['evidence'][0][:100]}...")
                else:
                    st.write("‚úÖ No issues detected")
            
            with col_rc:
                st.markdown("**Risk Compliance**")
                rc_hits = diagnostics_hits.get("risk_compliance", [])
                if rc_hits:
                    for hit in rc_hits:
                        st.markdown(f"‚Ä¢ **{hit['key']}**")
                        if hit.get('evidence'):
                            st.caption(f"Evidence: {hit['evidence'][0][:100]}...")
                else:
                    st.write("‚úÖ No issues detected")
            
            penalty_status = "Applied" if apply_diagnostics else "Display only"
            st.caption(f"Penalty status: {penalty_status}")
        else:
            st.info("üéâ No diagnostic issues detected!")
    elif not apply_diagnostics:
        st.info("‚ÑπÔ∏è Diagnostic penalties disabled")

    export = {
        "conversation_id": conv_id.strip(),
        "base_url": base_url,
        "evaluated_at": datetime.utcnow().isoformat() + "Z",
        "llm_model": llm_model.strip(),
        "brand": selected_brand,
        "rubric_version": rubrics_cfg.get("version", "v1.0"),
        "result": result.model_dump(),
        "metrics": metrics,
        "transcript_preview": transcript[:2000],
    }
    st.download_button(
        "‚¨áÔ∏è T·∫£i JSON k·∫øt qu·∫£",
        data=json.dumps(export, ensure_ascii=False, indent=2),
        file_name=f"evaluation_{conv_id.strip()}.json",
        mime="application/json"
    )