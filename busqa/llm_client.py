import json, re, time
import asyncio
from typing import Any, Dict
from concurrent.futures import ThreadPoolExecutor
from openai import AsyncOpenAI, OpenAI
import google.generativeai as genai

# Thread pool t·ªëi ∆∞u cho Docker (gi·ªõi h·∫°n theo cores) - ULTRA HIGH PERFORMANCE
import os
_max_workers = min(200, max(20, os.cpu_count() * 8))  # TƒÉng workers ƒë·ªÉ h·ªó tr·ª£ concurrency cao
_gemini_executor = ThreadPoolExecutor(max_workers=_max_workers, thread_name_prefix="gemini")
print(f"üöÄ Gemini ThreadPool initialized with {_max_workers} workers for ultra-high concurrency")

# Global OpenAI client v·ªõi connection pooling
_openai_client_cache = {}
_openai_async_client_cache = {}

def _get_openai_client(api_key: str, base_url: str = None) -> OpenAI:
    """Get cached OpenAI client v·ªõi connection pooling"""
    cache_key = f"{api_key[:10]}_{base_url or 'default'}"
    if cache_key not in _openai_client_cache:
        _openai_client_cache[cache_key] = OpenAI(
            api_key=api_key, 
            base_url=base_url,
            max_retries=0  # Handle retries manually for better control
        )
    return _openai_client_cache[cache_key]

def _get_async_openai_client(api_key: str, base_url: str = None) -> AsyncOpenAI:
    """Get cached AsyncOpenAI client v·ªõi connection pooling"""
    cache_key = f"{api_key[:10]}_{base_url or 'default'}"
    if cache_key not in _openai_async_client_cache:
        _openai_async_client_cache[cache_key] = AsyncOpenAI(
            api_key=api_key, 
            base_url=base_url,
            max_retries=0  # Handle retries manually
        )
    return _openai_async_client_cache[cache_key]

def call_llm(api_key: str, model: str, system_prompt: str, user_prompt: str, base_url: str | None = None, temperature: float = 0.2, max_retries: int = 3) -> Dict[str, Any]:
    """G·ªçi LLM v·ªõi retry t·ªëi ∆∞u cho batch processing"""
    if model.startswith("gemini"):
        genai.configure(api_key=api_key)
        generation_config = {
            "temperature": temperature,
            "response_mime_type": "application/json"
        }
        prompt = system_prompt + "\n\n" + user_prompt
        
        for attempt in range(max_retries + 1):
            try:
                model_obj = genai.GenerativeModel(model)
                resp = model_obj.generate_content(prompt, generation_config=generation_config)
                return json.loads(resp.text)
            except Exception as e:
                if attempt < max_retries:
                    time.sleep(0.5 * (attempt + 1))  # Gi·∫£m sleep time
                else:
                    raise e
    else:
        client = _get_openai_client(api_key, base_url)
        
        for attempt in range(max_retries + 1):
            try:
                resp = client.chat.completions.create(
                    model=model,
                    temperature=temperature,
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                )
                content = resp.choices[0].message.content
                return json.loads(content)
            except Exception as e:
                # Th·ª≠ parse partial JSON
                try:
                    if 'resp' in locals():
                        text = resp.choices[0].message.content or ""
                        m = re.search(r"\{.*\}", text, flags=re.S)
                        if m:
                            return json.loads(m.group(0))
                except:
                    pass
                
                if attempt < max_retries:
                    time.sleep(0.3 * (attempt + 1))  # Gi·∫£m sleep time
                else:
                    raise e


async def call_llm_async(api_key: str, model: str, system_prompt: str, user_prompt: str, base_url: str | None = None, temperature: float = 0.2, max_retries: int = 3) -> Dict[str, Any]:
    """Async version c·ªßa call_llm cho true concurrent processing"""
    if model.startswith("gemini"):
        genai.configure(api_key=api_key)
        generation_config = {
            "temperature": temperature,
            "response_mime_type": "application/json"
        }
        prompt = system_prompt + "\n\n" + user_prompt
        
        def _gemini_call():
            """Isolated Gemini call ƒë·ªÉ ch·∫°y trong thread pool ri√™ng"""
            model_obj = genai.GenerativeModel(model)
            return model_obj.generate_content(prompt, generation_config=generation_config)
        
        for attempt in range(max_retries + 1):
            try:
                # D√πng thread pool ri√™ng v·ªõi nhi·ªÅu workers - tƒÉng timeout
                loop = asyncio.get_event_loop()
                resp = await asyncio.wait_for(
                    loop.run_in_executor(_gemini_executor, _gemini_call),
                    timeout=30.0  # Timeout 30s cho m·ªói LLM call
                )
                return json.loads(resp.text)
            except asyncio.TimeoutError:
                if attempt < max_retries:
                    await asyncio.sleep(0.2)  # Gi·∫£m sleep time cho retry nhanh h∆°n
                else:
                    raise Exception(f"Gemini API timeout after 30s (attempt {attempt+1}/{max_retries+1})")
            except Exception as e:
                if attempt < max_retries:
                    await asyncio.sleep(0.2 * (attempt + 1))  # Gi·∫£m sleep time
                else:
                    raise e
    else:
        # S·ª≠ d·ª•ng cached AsyncOpenAI client cho true async v·ªõi connection pooling
        client = _get_async_openai_client(api_key, base_url)
        
        for attempt in range(max_retries + 1):
            try:
                resp = await client.chat.completions.create(
                    model=model,
                    temperature=temperature,
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                )
                content = resp.choices[0].message.content
                return json.loads(content)
            except Exception as e:
                # Th·ª≠ parse partial JSON
                try:
                    if 'resp' in locals():
                        text = resp.choices[0].message.content or ""
                        m = re.search(r"\{.*\}", text, flags=re.S)
                        if m:
                            return json.loads(m.group(0))
                except:
                    pass
                
                if attempt < max_retries:
                    await asyncio.sleep(0.3 * (attempt + 1))  # Async sleep
                else:
                    raise e